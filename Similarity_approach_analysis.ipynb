{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a63d0f0f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b157de51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "EpisodeID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "AlarmStart",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "AlarmEnd",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "AlarmDurationMinutes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "TotalWindowMinutes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "OperatedTags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "OperatedTagsCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "DeviatedTags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "DeviatedTagsCount",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "HasOperatorActions",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "HasOnlyTargetTags",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Has1071Action",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Has1016Action",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "Has1013Action",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "dbc4ec9f-8748-4d12-a242-d274e6ee7465",
       "rows": [
        [
         "0",
         "1",
         "2022-01-05 08:53:00",
         "2022-01-05 09:33:00",
         "40",
         "186",
         "03FIC_1085, 03FIC_3435, 03HIC_1141, 03HIC_1151, 03HIC_3100, 03LIC_1016, 03LIC_1031, 03LIC_1071, 03LIC_1085, 03LIC_1094, 03LIC_1097, 03PIC_1013, 03SDV_1167, 03XAX_1001, 03XAX_1002",
         "15",
         "02FI_1000.PV, 03FIC_1085.PV, 03FIC_3415.PV, 03FI_1151.PV, 03LIC_1016.PV, 03LIC_1071.PV, 03LIC_1085.PV, 03LIC_1094.PV, 03LIC_1097.PV, 03LIC_3178.PV, 03LI_3411.PV, 03PIC_1013.PV, 03PIC_1104.PV, 03PI_1141A.PV, 03PI_1495.PV, 03PI_1814.PV, 03TIC_1092.PV, 03TIC_1145.PV, 03TI_1015.PV, 03TI_1081.PV, 03TI_1421.PV, 03TI_1901.PV",
         "22",
         "True",
         "False",
         "True",
         "True",
         "True"
        ],
        [
         "1",
         "2",
         "2022-01-07 09:55:00",
         "2022-01-07 10:00:00",
         "5",
         "151",
         "03FIC_3435, 03GHS_0121A, 03GHS_0121AA, 03GHS_0121B, 03GM_0121, 03GM_0121A, 03HIC_3100, 04RES_3ER1AD",
         "8",
         "03FIC_1085.PV, 03FIC_3415.PV, 03FI_1141A.PV, 03FI_1151.PV, 03LIC_1016.PV, 03LIC_1071.PV, 03LIC_1085.PV, 03LIC_1094.PV, 03LIC_1097.PV, 03LIC_3178.PV, 03PIC_1013.PV, 03PIC_1068.PV, 03PIC_3131.PV, 03PI_1141A.PV, 03PI_1495.PV, 03PI_1814.PV, 03TIC_1092.PV, 03TIC_1142.PV, 03TIC_1145.PV, 03TI_1015.PV, 03TI_1081.PV, 03TI_1421.PV, 03TI_1901.PV",
         "23",
         "True",
         "False",
         "False",
         "False",
         "False"
        ],
        [
         "2",
         "3",
         "2022-01-07 13:33:00",
         "2022-01-07 13:36:00",
         "3",
         "149",
         "03FIC_3435, 03LIC_1034",
         "2",
         "03FIC_1085.PV, 03FIC_3415.PV, 03FI_1141A.PV, 03FI_1151.PV, 03LIC_1016.PV, 03LIC_1071.PV, 03LIC_1094.PV, 03LIC_1097.PV, 03LIC_3178.PV, 03PIC_1013.PV, 03PIC_1104.PV, 03PI_1141A.PV, 03PI_1495.PV, 03PI_1814.PV, 03TIC_1092.PV, 03TIC_1145.PV, 03TI_1015.PV, 03TI_1081.PV, 03TI_1421.PV",
         "19",
         "True",
         "False",
         "False",
         "False",
         "False"
        ],
        [
         "3",
         "4",
         "2022-01-07 14:17:00",
         "2022-01-07 14:19:00",
         "2",
         "148",
         "03FIC_3435, 03LIC_1016, 03LIC_1034",
         "3",
         "03FIC_1085.PV, 03FI_1141A.PV, 03LIC_1016.PV, 03LIC_1071.PV, 03LIC_1085.PV, 03LIC_1094.PV, 03LIC_1097.PV, 03LIC_3178.PV, 03PIC_1013.PV, 03PIC_1068.PV, 03PIC_1104.PV, 03PIC_3131.PV, 03PI_1141A.PV, 03PI_1495.PV, 03PI_1814.PV, 03TIC_1092.PV, 03TIC_1142.PV, 03TIC_1145.PV, 03TI_1015.PV, 03TI_1081.PV, 03TI_1421.PV, 03TI_1901.PV",
         "22",
         "True",
         "False",
         "False",
         "True",
         "False"
        ],
        [
         "4",
         "5",
         "2022-01-07 14:54:00",
         "2022-01-07 14:58:00",
         "4",
         "150",
         "03FIC_3435, 03LIC_1016",
         "2",
         "02FI_1000.PV, 03FIC_1085.PV, 03FI_1141A.PV, 03LIC_1016.PV, 03LIC_1071.PV, 03LIC_1094.PV, 03LIC_1097.PV, 03PIC_1013.PV, 03PIC_1068.PV, 03PIC_3131.PV, 03PI_1141A.PV, 03PI_1495.PV, 03PI_1814.PV, 03TIC_1142.PV, 03TIC_1145.PV, 03TI_1015.PV, 03TI_1081.PV, 03TI_1421.PV, 03TI_1901.PV",
         "19",
         "True",
         "False",
         "False",
         "True",
         "False"
        ]
       ],
       "shape": {
        "columns": 14,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EpisodeID</th>\n",
       "      <th>AlarmStart</th>\n",
       "      <th>AlarmEnd</th>\n",
       "      <th>AlarmDurationMinutes</th>\n",
       "      <th>TotalWindowMinutes</th>\n",
       "      <th>OperatedTags</th>\n",
       "      <th>OperatedTagsCount</th>\n",
       "      <th>DeviatedTags</th>\n",
       "      <th>DeviatedTagsCount</th>\n",
       "      <th>HasOperatorActions</th>\n",
       "      <th>HasOnlyTargetTags</th>\n",
       "      <th>Has1071Action</th>\n",
       "      <th>Has1016Action</th>\n",
       "      <th>Has1013Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-01-05 08:53:00</td>\n",
       "      <td>2022-01-05 09:33:00</td>\n",
       "      <td>40</td>\n",
       "      <td>186</td>\n",
       "      <td>03FIC_1085, 03FIC_3435, 03HIC_1141, 03HIC_1151...</td>\n",
       "      <td>15</td>\n",
       "      <td>02FI_1000.PV, 03FIC_1085.PV, 03FIC_3415.PV, 03...</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-01-07 09:55:00</td>\n",
       "      <td>2022-01-07 10:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>151</td>\n",
       "      <td>03FIC_3435, 03GHS_0121A, 03GHS_0121AA, 03GHS_0...</td>\n",
       "      <td>8</td>\n",
       "      <td>03FIC_1085.PV, 03FIC_3415.PV, 03FI_1141A.PV, 0...</td>\n",
       "      <td>23</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-01-07 13:33:00</td>\n",
       "      <td>2022-01-07 13:36:00</td>\n",
       "      <td>3</td>\n",
       "      <td>149</td>\n",
       "      <td>03FIC_3435, 03LIC_1034</td>\n",
       "      <td>2</td>\n",
       "      <td>03FIC_1085.PV, 03FIC_3415.PV, 03FI_1141A.PV, 0...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-07 14:17:00</td>\n",
       "      <td>2022-01-07 14:19:00</td>\n",
       "      <td>2</td>\n",
       "      <td>148</td>\n",
       "      <td>03FIC_3435, 03LIC_1016, 03LIC_1034</td>\n",
       "      <td>3</td>\n",
       "      <td>03FIC_1085.PV, 03FI_1141A.PV, 03LIC_1016.PV, 0...</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-07 14:54:00</td>\n",
       "      <td>2022-01-07 14:58:00</td>\n",
       "      <td>4</td>\n",
       "      <td>150</td>\n",
       "      <td>03FIC_3435, 03LIC_1016</td>\n",
       "      <td>2</td>\n",
       "      <td>02FI_1000.PV, 03FIC_1085.PV, 03FI_1141A.PV, 03...</td>\n",
       "      <td>19</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EpisodeID          AlarmStart            AlarmEnd  AlarmDurationMinutes  \\\n",
       "0          1 2022-01-05 08:53:00 2022-01-05 09:33:00                    40   \n",
       "1          2 2022-01-07 09:55:00 2022-01-07 10:00:00                     5   \n",
       "2          3 2022-01-07 13:33:00 2022-01-07 13:36:00                     3   \n",
       "3          4 2022-01-07 14:17:00 2022-01-07 14:19:00                     2   \n",
       "4          5 2022-01-07 14:54:00 2022-01-07 14:58:00                     4   \n",
       "\n",
       "   TotalWindowMinutes                                       OperatedTags  \\\n",
       "0                 186  03FIC_1085, 03FIC_3435, 03HIC_1141, 03HIC_1151...   \n",
       "1                 151  03FIC_3435, 03GHS_0121A, 03GHS_0121AA, 03GHS_0...   \n",
       "2                 149                             03FIC_3435, 03LIC_1034   \n",
       "3                 148                 03FIC_3435, 03LIC_1016, 03LIC_1034   \n",
       "4                 150                             03FIC_3435, 03LIC_1016   \n",
       "\n",
       "   OperatedTagsCount                                       DeviatedTags  \\\n",
       "0                 15  02FI_1000.PV, 03FIC_1085.PV, 03FIC_3415.PV, 03...   \n",
       "1                  8  03FIC_1085.PV, 03FIC_3415.PV, 03FI_1141A.PV, 0...   \n",
       "2                  2  03FIC_1085.PV, 03FIC_3415.PV, 03FI_1141A.PV, 0...   \n",
       "3                  3  03FIC_1085.PV, 03FI_1141A.PV, 03LIC_1016.PV, 0...   \n",
       "4                  2  02FI_1000.PV, 03FIC_1085.PV, 03FI_1141A.PV, 03...   \n",
       "\n",
       "   DeviatedTagsCount  HasOperatorActions  HasOnlyTargetTags  Has1071Action  \\\n",
       "0                 22                True              False           True   \n",
       "1                 23                True              False          False   \n",
       "2                 19                True              False          False   \n",
       "3                 22                True              False          False   \n",
       "4                 19                True              False          False   \n",
       "\n",
       "   Has1016Action  Has1013Action  \n",
       "0           True           True  \n",
       "1          False          False  \n",
       "2          False          False  \n",
       "3           True          False  \n",
       "4           True          False  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "episodes_operated_tags_df = pd.read_excel('/home/h604827/ControlActions/RESULTS/episode_all_operator_action_plots/episodes_all_with_actions_and_deviations.xlsx')\n",
    "ssd_df = pd.read_excel('/home/h604827/ControlActions/DATA/SSD_1071_SSD_output_1071_7Jan2026.xlsx')\n",
    "events_df = pd.read_csv('/home/h604827/ControlActions/DATA/trip_filtered_events.csv')\n",
    "pv_op_data_df = pd.read_parquet('/home/h604827/ControlActions/DATA/03LIC_1071_JAN_2026_filtered.parquet')\n",
    "operating_limits_df = pd.read_csv('/home/h604827/ControlActions/DATA/operating_limits.csv')\n",
    "\n",
    "episodes_operated_tags_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae409b11",
   "metadata": {},
   "source": [
    "### Checking operator actions for target tags only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "37f88fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Description",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "b5e9ae9d-539f-4a9f-8992-b6f1a5a2fde2",
       "rows": [
        [
         "OP",
         "162020"
        ],
        [
         "SP",
         "55104"
        ],
        [
         "MODE",
         "11200"
        ],
        [
         "SO",
         "1700"
        ],
        [
         "PVFL",
         "1110"
        ],
        [
         "PVSOURCE",
         "214"
        ],
        [
         "PVSRCOPT",
         "158"
        ],
        [
         "OROPT",
         "54"
        ],
        [
         "BYPASS",
         "34"
        ],
        [
         "PV",
         "28"
        ],
        [
         "PVHITP",
         "24"
        ],
        [
         "PTEXECST",
         "18"
        ],
        [
         "PVHIPR",
         "12"
        ],
        [
         "COMMAND",
         "10"
        ],
        [
         "PVTV",
         "4"
        ],
        [
         "MODATTR",
         "4"
        ],
        [
         "ASSOCDSP",
         "4"
        ],
        [
         "FL(09)",
         "4"
        ],
        [
         "DLYTIME",
         "2"
        ],
        [
         "PVEXEUHI",
         "2"
        ],
        [
         "RCASOPT",
         "2"
        ],
        [
         "C",
         "2"
        ],
        [
         "TF",
         "2"
        ],
        [
         "PTDESC",
         "2"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 24
       }
      },
      "text/plain": [
       "Description\n",
       "OP          162020\n",
       "SP           55104\n",
       "MODE         11200\n",
       "SO            1700\n",
       "PVFL          1110\n",
       "PVSOURCE       214\n",
       "PVSRCOPT       158\n",
       "OROPT           54\n",
       "BYPASS          34\n",
       "PV              28\n",
       "PVHITP          24\n",
       "PTEXECST        18\n",
       "PVHIPR          12\n",
       "COMMAND         10\n",
       "PVTV             4\n",
       "MODATTR          4\n",
       "ASSOCDSP         4\n",
       "FL(09)           4\n",
       "DLYTIME          2\n",
       "PVEXEUHI         2\n",
       "RCASOPT          2\n",
       "C                2\n",
       "TF               2\n",
       "PTDESC           2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events_df[events_df['ConditionName'] == 'CHANGE']['Description'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35e03a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total CHANGE events for target tags: 20504\n",
      "\n",
      "Sample CHANGE events:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "VT_Start",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Value",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PrevValue",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "ConditionName",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "50a74bec-c05c-4b44-87f6-0e3fb003f455",
       "rows": [
        [
         "2512",
         "03PIC_1013",
         "2021-10-05 15:16:02.478400",
         "MAN",
         "CAS",
         "CHANGE"
        ],
        [
         "2513",
         "03PIC_1013",
         "2021-10-05 15:16:02.478400",
         "MAN",
         null,
         "CHANGE"
        ],
        [
         "2514",
         "03PIC_1013",
         "2021-10-05 15:16:05.255600",
         "87.9999",
         null,
         "CHANGE"
        ],
        [
         "2516",
         "03PIC_1013",
         "2021-10-05 15:16:05.255600",
         "87.9999",
         "89.9999",
         "CHANGE"
        ],
        [
         "2518",
         "03PIC_1013",
         "2021-10-05 15:16:07.755600",
         "85.9998",
         "87.9999",
         "CHANGE"
        ],
        [
         "2519",
         "03PIC_1013",
         "2021-10-05 15:16:07.755600",
         "85.9998",
         null,
         "CHANGE"
        ],
        [
         "2520",
         "03PIC_1013",
         "2021-10-05 15:16:21.711300",
         "83.9999",
         null,
         "CHANGE"
        ],
        [
         "2522",
         "03PIC_1013",
         "2021-10-05 15:16:21.711300",
         "83.9999",
         "85.9998",
         "CHANGE"
        ],
        [
         "2524",
         "03PIC_1013",
         "2021-10-05 15:16:44.655600",
         "81.9999",
         "83.9999",
         "CHANGE"
        ],
        [
         "2525",
         "03PIC_1013",
         "2021-10-05 15:16:44.655600",
         "81.9999",
         null,
         "CHANGE"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>VT_Start</th>\n",
       "      <th>Value</th>\n",
       "      <th>PrevValue</th>\n",
       "      <th>ConditionName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:02.478400</td>\n",
       "      <td>MAN</td>\n",
       "      <td>CAS</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:02.478400</td>\n",
       "      <td>MAN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2514</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:05.255600</td>\n",
       "      <td>87.9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2516</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:05.255600</td>\n",
       "      <td>87.9999</td>\n",
       "      <td>89.9999</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2518</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:07.755600</td>\n",
       "      <td>85.9998</td>\n",
       "      <td>87.9999</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2519</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:07.755600</td>\n",
       "      <td>85.9998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2520</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:21.711300</td>\n",
       "      <td>83.9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:21.711300</td>\n",
       "      <td>83.9999</td>\n",
       "      <td>85.9998</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:44.655600</td>\n",
       "      <td>81.9999</td>\n",
       "      <td>83.9999</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>2021-10-05 15:16:44.655600</td>\n",
       "      <td>81.9999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CHANGE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Source                    VT_Start    Value PrevValue ConditionName\n",
       "2512  03PIC_1013  2021-10-05 15:16:02.478400      MAN       CAS        CHANGE\n",
       "2513  03PIC_1013  2021-10-05 15:16:02.478400      MAN       NaN        CHANGE\n",
       "2514  03PIC_1013  2021-10-05 15:16:05.255600  87.9999       NaN        CHANGE\n",
       "2516  03PIC_1013  2021-10-05 15:16:05.255600  87.9999   89.9999        CHANGE\n",
       "2518  03PIC_1013  2021-10-05 15:16:07.755600  85.9998   87.9999        CHANGE\n",
       "2519  03PIC_1013  2021-10-05 15:16:07.755600  85.9998       NaN        CHANGE\n",
       "2520  03PIC_1013  2021-10-05 15:16:21.711300  83.9999       NaN        CHANGE\n",
       "2522  03PIC_1013  2021-10-05 15:16:21.711300  83.9999   85.9998        CHANGE\n",
       "2524  03PIC_1013  2021-10-05 15:16:44.655600  81.9999   83.9999        CHANGE\n",
       "2525  03PIC_1013  2021-10-05 15:16:44.655600  81.9999       NaN        CHANGE"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how operator actions (CHANGE events) look for target tags\n",
    "# Filter for CHANGE events on target tags (1071, 1016, 1013)\n",
    "target_sources = ['03LIC_1071', '03LIC_1016', '03PIC_1013']\n",
    "change_events = events_df[(events_df['ConditionName'] == 'CHANGE') & \n",
    "                          (events_df['Source'].isin(target_sources))]\n",
    "print(f\"Total CHANGE events for target tags: {len(change_events)}\")\n",
    "print(\"\\nSample CHANGE events:\")\n",
    "change_events[['Source', 'VT_Start', 'Value', 'PrevValue', 'ConditionName']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f883a",
   "metadata": {},
   "source": [
    "## Similarity Approach: Building Context for Operator Actions\n",
    "\n",
    "### Approach Overview\n",
    "1. Select 20 random episodes from 27 episodes where only target tags (1071, 1016, 1013) were operated\n",
    "2. For each operator action in these episodes, create a \"context\" capturing process state\n",
    "3. Context window: From deviation start time to operator action timestamp\n",
    "\n",
    "### Context Features (for each of 28 PV tags):\n",
    "1. **PV value at deviation start**\n",
    "2. **PV value at operator action time**\n",
    "3. **Rate of change (%)** = (PV_action - PV_start) / PV_start * 100\n",
    "4. **Direction** = 1 if ROC positive, 0 if negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b8600ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total episodes with only target tags: 27\n",
      "Selected 20 episodes for training context\n",
      "\n",
      "Selected Episode IDs: [397, 468, 399, 526, 295, 407, 473, 475, 413, 529, 359, 371, 377, 360, 470, 527, 363, 560, 528, 483]\n",
      "\n",
      "Remaining 7 episodes for testing: [386, 388, 400, 469, 521, 522, 604]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter episodes where only target tags were operated\n",
    "only_target_episodes_df = episodes_operated_tags_df[episodes_operated_tags_df['HasOnlyTargetTags'] == True].copy()\n",
    "print(f\"Total episodes with only target tags: {len(only_target_episodes_df)}\")\n",
    "\n",
    "# Step 2: Randomly select 20 episodes (set seed for reproducibility)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "sample_n = 20\n",
    "selected_episodes_df = only_target_episodes_df.sample(n=sample_n, random_state=42)\n",
    "print(f\"Selected {sample_n} episodes for training context\")\n",
    "print(f\"\\nSelected Episode IDs: {selected_episodes_df['EpisodeID'].tolist()}\")\n",
    "\n",
    "# Keep remaining 7 for later use (testing)\n",
    "remaining_episodes_df = only_target_episodes_df[~only_target_episodes_df['EpisodeID'].isin(selected_episodes_df['EpisodeID'])]\n",
    "print(f\"\\nRemaining {len(remaining_episodes_df)} episodes for testing: {remaining_episodes_df['EpisodeID'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "46e02fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "OperatedTags",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cee71781-8b8f-41db-bb03-8ce7e62af63e",
       "rows": [
        [
         "03LIC_1071",
         "11"
        ],
        [
         "03PIC_1013",
         "6"
        ],
        [
         "03LIC_1016, 03LIC_1071",
         "3"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "OperatedTags\n",
       "03LIC_1071                11\n",
       "03PIC_1013                 6\n",
       "03LIC_1016, 03LIC_1071     3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_episodes_df['OperatedTags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "adfe4a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp columns converted to datetime\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define the PV columns for context (28 tags)\n",
    "\n",
    "context_pv_tags = [col for col in pv_op_data_df.columns if col.endswith('.PV')]\n",
    "\n",
    "# Step 4: Prepare events data for timestamp parsing\n",
    "events_df['VT_Start'] = pd.to_datetime(events_df['VT_Start'])\n",
    "\n",
    "# Prepare SSD data for timestamp parsing\n",
    "ssd_df['AlarmStart_rounded_minutes'] = pd.to_datetime(ssd_df['AlarmStart_rounded_minutes'])\n",
    "ssd_df['AlarmEnd_rounded_minutes'] = pd.to_datetime(ssd_df['AlarmEnd_rounded_minutes'])\n",
    "ssd_df['Tag_First_Transition_Start_minutes'] = pd.to_datetime(ssd_df['Tag_First_Transition_Start_minutes'])\n",
    "\n",
    "# Prepare episodes data\n",
    "episodes_operated_tags_df['AlarmStart'] = pd.to_datetime(episodes_operated_tags_df['AlarmStart'])\n",
    "episodes_operated_tags_df['AlarmEnd'] = pd.to_datetime(episodes_operated_tags_df['AlarmEnd'])\n",
    "\n",
    "print(\"Timestamp columns converted to datetime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eee02256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 397:\n",
      "  Alarm Start: 2024-06-16 10:29:00\n",
      "  Deviation Start: 2024-06-16 09:03:00\n",
      "  Alarm End: 2024-06-16 10:50:00\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Helper function to get deviation start time for an episode\n",
    "# Deviation start is the earliest Tag_First_Transition_Start_minutes for the target tag (03LIC_1071)\n",
    "# in that alarm episode\n",
    "\n",
    "def get_deviation_start_for_episode(episode_id, alarm_start, alarm_end):\n",
    "    \"\"\"\n",
    "    Get the deviation start time for an episode.\n",
    "    This is the Tag_First_Transition_Start_minutes for 03LIC_1071 (or earliest among related tags)\n",
    "    \"\"\"\n",
    "    # Find SSD records for this alarm episode (matching by alarm start time)\n",
    "    episode_ssd = ssd_df[\n",
    "        (ssd_df['AlarmStart_rounded_minutes'] == alarm_start)\n",
    "    ]\n",
    "    \n",
    "    if len(episode_ssd) == 0:\n",
    "        # Try with a small time tolerance (within 1 minute)\n",
    "        print(f\"No exact SSD data found for EpisodeID {episode_id} with AlarmStart {alarm_start}. Trying with time tolerance.\")\n",
    "        episode_ssd = ssd_df[\n",
    "            (abs((ssd_df['AlarmStart_rounded_minutes'] - alarm_start).dt.total_seconds()) <= 60)\n",
    "        ]\n",
    "    \n",
    "    if len(episode_ssd) == 0:\n",
    "        # If no SSD data found, use alarm start minus 30 minutes as default deviation start\n",
    "        print(f\"No SSD data found for EpisodeID {episode_id} with AlarmStart {alarm_start}. Using default deviation start.\")\n",
    "        return alarm_start - pd.Timedelta(minutes=30)\n",
    "    \n",
    "    # Get deviation start for target tag 03LIC_1071 if available\n",
    "    target_ssd = episode_ssd[episode_ssd['TagName'] == '03LIC_1071']\n",
    "    if len(target_ssd) > 0:\n",
    "        return target_ssd['Tag_First_Transition_Start_minutes'].iloc[0]\n",
    "    \n",
    "    # If not found, use earliest transition start among all tags\n",
    "    return episode_ssd['Tag_First_Transition_Start_minutes'].min()\n",
    "\n",
    "# Test with first selected episode\n",
    "test_ep = selected_episodes_df.iloc[0]\n",
    "test_dev_start = get_deviation_start_for_episode(\n",
    "    test_ep['EpisodeID'], \n",
    "    test_ep['AlarmStart'], \n",
    "    test_ep['AlarmEnd']\n",
    ")\n",
    "print(f\"Episode {test_ep['EpisodeID']}:\")\n",
    "print(f\"  Alarm Start: {test_ep['AlarmStart']}\")\n",
    "print(f\"  Deviation Start: {test_dev_start}\")\n",
    "print(f\"  Alarm End: {test_ep['AlarmEnd']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6ca45e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 397 - Found 24 operator actions\n",
      "            Source                   VT_Start    Value PrevValue\n",
      "991844  03PIC_1013 2024-06-16 09:18:57.165100  82.9997   84.9997\n",
      "991845  03PIC_1013 2024-06-16 09:18:57.165100  82.9997       NaN\n",
      "991846  03PIC_1013 2024-06-16 09:18:58.805800  80.9997       NaN\n",
      "991847  03PIC_1013 2024-06-16 09:18:58.805800  80.9997   82.9997\n",
      "991849  03PIC_1013 2024-06-16 09:19:04.305600  78.9997   80.9997\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Helper function to get operator actions for an episode\n",
    "def get_operator_actions_for_episode(alarm_start, alarm_end, target_sources=['03LIC_1071', '03LIC_1016', '03PIC_1013']):\n",
    "    \"\"\"\n",
    "    Get all CHANGE events (operator actions) for target sources during an episode.\n",
    "    Episode window: deviation_start to alarm_end\n",
    "    \"\"\"\n",
    "    # Get deviation start\n",
    "    deviation_start = get_deviation_start_for_episode(None, alarm_start, alarm_end)\n",
    "    \n",
    "    # Filter CHANGE events within the episode window for target sources\n",
    "    actions = events_df[\n",
    "        (events_df['ConditionName'] == 'CHANGE') &\n",
    "        (events_df['Source'].isin(target_sources)) &\n",
    "        (events_df['VT_Start'] >= deviation_start) &\n",
    "        (events_df['VT_Start'] <= alarm_end)\n",
    "    ].copy()\n",
    "    \n",
    "    return actions, deviation_start\n",
    "\n",
    "# Test with first selected episode\n",
    "test_actions, test_dev_start = get_operator_actions_for_episode(\n",
    "    test_ep['AlarmStart'], \n",
    "    test_ep['AlarmEnd']\n",
    ")\n",
    "print(f\"Episode {test_ep['EpisodeID']} - Found {len(test_actions)} operator actions\")\n",
    "if len(test_actions) > 0:\n",
    "    print(test_actions[['Source', 'VT_Start', 'Value', 'PrevValue']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d50858fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV value for 03LIC_1071.PV at 2024-06-16 09:03:00: 40.202705\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Helper function to get PV value at a specific timestamp (with nearest lookup)\n",
    "def get_pv_at_timestamp(timestamp, pv_tag):\n",
    "    \"\"\"\n",
    "    Get PV value at or nearest to the given timestamp.\n",
    "    Uses forward fill to get the most recent value if exact time not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Make timestamp timezone naive if needed\n",
    "        if timestamp.tzinfo is not None:\n",
    "            timestamp = timestamp.tz_localize(None)\n",
    "        \n",
    "        # Try exact lookup first\n",
    "        if timestamp in pv_op_data_df.index:\n",
    "            return pv_op_data_df.loc[timestamp, pv_tag]\n",
    "        \n",
    "        # Use asof for nearest lookup (gets value at or before timestamp)\n",
    "        idx = pv_op_data_df.index.get_indexer([timestamp], method='ffill')[0]\n",
    "        if idx >= 0 and idx < len(pv_op_data_df):\n",
    "            return pv_op_data_df.iloc[idx][pv_tag]\n",
    "        \n",
    "        # If no value found before, get nearest after\n",
    "        idx = pv_op_data_df.index.get_indexer([timestamp], method='bfill')[0]\n",
    "        if idx >= 0 and idx < len(pv_op_data_df):\n",
    "            return pv_op_data_df.iloc[idx][pv_tag]\n",
    "        \n",
    "        return np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting PV value for {pv_tag} at {timestamp}: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "# Test\n",
    "test_ts = test_dev_start\n",
    "test_tag = '03LIC_1071.PV'\n",
    "test_val = get_pv_at_timestamp(test_ts, test_tag)\n",
    "print(f\"PV value for {test_tag} at {test_ts}: {test_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb68aff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context for action at 2024-06-16 09:18:57.165100 on 03PIC_1013:\n",
      "\n",
      "03LIC_1071 (target tag):\n",
      "  PV at deviation start: 40.2027\n",
      "  PV at action: 40.1849\n",
      "  ROC %: -0.0442\n",
      "  ROC direction: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Build context for a single operator action\n",
    "def build_context_for_action(deviation_start, action_timestamp, pv_tags):\n",
    "    \"\"\"\n",
    "    Build context features for an operator action.\n",
    "    \n",
    "    Returns a dict with the following for each PV tag:\n",
    "    - {tag}_pv_at_deviation_start: PV value at deviation start\n",
    "    - {tag}_pv_at_action: PV value at action time\n",
    "    - {tag}_roc_percent: Rate of change in % = (pv_action - pv_start) / pv_start * 100\n",
    "    - {tag}_roc_direction: 1 if positive, 0 if negative\n",
    "    \"\"\"\n",
    "    context = {}\n",
    "    \n",
    "    for pv_tag in pv_tags:\n",
    "        # Remove .PV suffix for cleaner column names\n",
    "        tag_name = pv_tag.replace('.PV', '')\n",
    "        \n",
    "        # Get PV values\n",
    "        pv_at_deviation = get_pv_at_timestamp(deviation_start, pv_tag)\n",
    "        pv_at_action = get_pv_at_timestamp(action_timestamp, pv_tag)\n",
    "        \n",
    "        # Calculate rate of change (%)\n",
    "        if pd.notna(pv_at_deviation) and pd.notna(pv_at_action) and pv_at_deviation != 0:\n",
    "            roc_percent = ((pv_at_action - pv_at_deviation) / pv_at_deviation) * 100\n",
    "        else:\n",
    "            roc_percent = np.nan\n",
    "        \n",
    "        # Determine direction\n",
    "        if pd.notna(roc_percent):\n",
    "            roc_direction = 1 if roc_percent >= 0 else 0\n",
    "        else:\n",
    "            roc_direction = np.nan\n",
    "        \n",
    "        # Store in context\n",
    "        context[f'{tag_name}_pv_at_deviation_start'] = pv_at_deviation\n",
    "        context[f'{tag_name}_pv_at_action'] = pv_at_action\n",
    "        context[f'{tag_name}_roc_percent'] = roc_percent\n",
    "        context[f'{tag_name}_roc_direction'] = roc_direction\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Test with first action of first episode\n",
    "if len(test_actions) > 0:\n",
    "    first_action = test_actions.iloc[0]\n",
    "    test_context = build_context_for_action(test_dev_start, first_action['VT_Start'], context_pv_tags)\n",
    "    print(f\"Context for action at {first_action['VT_Start']} on {first_action['Source']}:\")\n",
    "    print(f\"\\n03LIC_1071 (target tag):\")\n",
    "    print(f\"  PV at deviation start: {test_context['03LIC_1071_pv_at_deviation_start']:.4f}\")\n",
    "    print(f\"  PV at action: {test_context['03LIC_1071_pv_at_action']:.4f}\")\n",
    "    print(f\"  ROC %: {test_context['03LIC_1071_roc_percent']:.4f}\")\n",
    "    print(f\"  ROC direction: {test_context['03LIC_1071_roc_direction']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2f48347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 20 selected episodes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:25<00:00,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total context records created: 468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Process all 20 selected episodes and build context for each operator action\n",
    "from tqdm import tqdm\n",
    "\n",
    "context_records = []\n",
    "target_sources = ['03LIC_1071', '03LIC_1016', '03PIC_1013']\n",
    "\n",
    "print(\"Processing 20 selected episodes...\")\n",
    "for idx, (_, episode) in enumerate(tqdm(selected_episodes_df.iterrows(), total=len(selected_episodes_df))):\n",
    "    episode_id = episode['EpisodeID']\n",
    "    alarm_start = episode['AlarmStart']\n",
    "    alarm_end = episode['AlarmEnd']\n",
    "    \n",
    "    # Get deviation start\n",
    "    deviation_start = get_deviation_start_for_episode(episode_id, alarm_start, alarm_end)\n",
    "    \n",
    "    # Get operator actions for this episode\n",
    "    actions, _ = get_operator_actions_for_episode(alarm_start, alarm_end, target_sources)\n",
    "    \n",
    "    if len(actions) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Process each operator action\n",
    "    for _, action in actions.iterrows():\n",
    "        action_timestamp = action['VT_Start']\n",
    "        \n",
    "        # Build context\n",
    "        context = build_context_for_action(deviation_start, action_timestamp, context_pv_tags)\n",
    "        \n",
    "        # Add episode and action metadata\n",
    "        context['episode_id'] = episode_id\n",
    "        context['alarm_start'] = alarm_start\n",
    "        context['alarm_end'] = alarm_end\n",
    "        context['deviation_start'] = deviation_start\n",
    "        context['action_timestamp'] = action_timestamp\n",
    "        context['action_source'] = action['Source']\n",
    "        context['action_value'] = action['Value']\n",
    "        context['action_prev_value'] = action['PrevValue']\n",
    "        \n",
    "        # Calculate action direction and magnitude\n",
    "        try:\n",
    "            action_val = float(action['Value'])\n",
    "            prev_val = float(action['PrevValue'])\n",
    "            context['action_magnitude'] = action_val - prev_val\n",
    "            context['action_direction'] = 1 if action_val > prev_val else 0  # 1 = increase, 0 = decrease\n",
    "        except (ValueError, TypeError):\n",
    "            context['action_magnitude'] = np.nan\n",
    "            context['action_direction'] = np.nan\n",
    "        \n",
    "        context_records.append(context)\n",
    "\n",
    "print(f\"\\nTotal context records created: {len(context_records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fcef5685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context DataFrame shape: (468, 122)\n",
      "Total columns: 122\n",
      "\n",
      "Column breakdown:\n",
      "  - Metadata columns: 10 (episode_id, alarm_start, alarm_end, deviation_start, action_timestamp, action_source, action_value, action_prev_value, action_magnitude, action_direction)\n",
      "  - PV context columns: 112 (4 features x 28 tags)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "episode_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "action_timestamp",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "action_source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "action_direction",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "action_magnitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "03LIC_1071_pv_at_deviation_start",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "03LIC_1071_pv_at_action",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "03LIC_1071_roc_percent",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "03LIC_1071_roc_direction",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "52177f75-0e86-4ace-ae01-008810698f2f",
       "rows": [
        [
         "0",
         "397",
         "2024-06-16 09:18:57.165100",
         "03PIC_1013",
         "0.0",
         "-2.0",
         "40.202705",
         "40.184917",
         "-0.044245778984282336",
         "0"
        ],
        [
         "1",
         "397",
         "2024-06-16 09:18:57.165100",
         "03PIC_1013",
         "0.0",
         null,
         "40.202705",
         "40.184917",
         "-0.044245778984282336",
         "0"
        ],
        [
         "2",
         "397",
         "2024-06-16 09:18:58.805800",
         "03PIC_1013",
         "0.0",
         null,
         "40.202705",
         "40.184917",
         "-0.044245778984282336",
         "0"
        ],
        [
         "3",
         "397",
         "2024-06-16 09:18:58.805800",
         "03PIC_1013",
         "0.0",
         "-2.0",
         "40.202705",
         "40.184917",
         "-0.044245778984282336",
         "0"
        ],
        [
         "4",
         "397",
         "2024-06-16 09:19:04.305600",
         "03PIC_1013",
         "0.0",
         "-2.0",
         "40.202705",
         "38.263474",
         "-4.823633136128525",
         "0"
        ],
        [
         "5",
         "397",
         "2024-06-16 09:19:04.305600",
         "03PIC_1013",
         "0.0",
         null,
         "40.202705",
         "38.263474",
         "-4.823633136128525",
         "0"
        ],
        [
         "6",
         "397",
         "2024-06-16 09:54:47.056800",
         "03PIC_1013",
         "0.0",
         "-2.0",
         "40.202705",
         "38.953682",
         "-3.106813335072854",
         "0"
        ],
        [
         "7",
         "397",
         "2024-06-16 09:54:47.056800",
         "03PIC_1013",
         "0.0",
         null,
         "40.202705",
         "38.953682",
         "-3.106813335072854",
         "0"
        ],
        [
         "8",
         "397",
         "2024-06-16 09:54:48.356000",
         "03PIC_1013",
         "0.0",
         null,
         "40.202705",
         "38.953682",
         "-3.106813335072854",
         "0"
        ],
        [
         "9",
         "397",
         "2024-06-16 09:54:48.356000",
         "03PIC_1013",
         "0.0",
         "-2.0",
         "40.202705",
         "38.953682",
         "-3.106813335072854",
         "0"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>action_timestamp</th>\n",
       "      <th>action_source</th>\n",
       "      <th>action_direction</th>\n",
       "      <th>action_magnitude</th>\n",
       "      <th>03LIC_1071_pv_at_deviation_start</th>\n",
       "      <th>03LIC_1071_pv_at_action</th>\n",
       "      <th>03LIC_1071_roc_percent</th>\n",
       "      <th>03LIC_1071_roc_direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:18:57.165100</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>40.184917</td>\n",
       "      <td>-0.044246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:18:57.165100</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>40.184917</td>\n",
       "      <td>-0.044246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:18:58.805800</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>40.184917</td>\n",
       "      <td>-0.044246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:18:58.805800</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>40.184917</td>\n",
       "      <td>-0.044246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:19:04.305600</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>38.263474</td>\n",
       "      <td>-4.823633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:19:04.305600</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>38.263474</td>\n",
       "      <td>-4.823633</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:54:47.056800</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>38.953682</td>\n",
       "      <td>-3.106813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:54:47.056800</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>38.953682</td>\n",
       "      <td>-3.106813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:54:48.356000</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>38.953682</td>\n",
       "      <td>-3.106813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>397</td>\n",
       "      <td>2024-06-16 09:54:48.356000</td>\n",
       "      <td>03PIC_1013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>40.202705</td>\n",
       "      <td>38.953682</td>\n",
       "      <td>-3.106813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episode_id           action_timestamp action_source  action_direction  \\\n",
       "0         397 2024-06-16 09:18:57.165100    03PIC_1013               0.0   \n",
       "1         397 2024-06-16 09:18:57.165100    03PIC_1013               0.0   \n",
       "2         397 2024-06-16 09:18:58.805800    03PIC_1013               0.0   \n",
       "3         397 2024-06-16 09:18:58.805800    03PIC_1013               0.0   \n",
       "4         397 2024-06-16 09:19:04.305600    03PIC_1013               0.0   \n",
       "5         397 2024-06-16 09:19:04.305600    03PIC_1013               0.0   \n",
       "6         397 2024-06-16 09:54:47.056800    03PIC_1013               0.0   \n",
       "7         397 2024-06-16 09:54:47.056800    03PIC_1013               0.0   \n",
       "8         397 2024-06-16 09:54:48.356000    03PIC_1013               0.0   \n",
       "9         397 2024-06-16 09:54:48.356000    03PIC_1013               0.0   \n",
       "\n",
       "   action_magnitude  03LIC_1071_pv_at_deviation_start  \\\n",
       "0              -2.0                         40.202705   \n",
       "1               NaN                         40.202705   \n",
       "2               NaN                         40.202705   \n",
       "3              -2.0                         40.202705   \n",
       "4              -2.0                         40.202705   \n",
       "5               NaN                         40.202705   \n",
       "6              -2.0                         40.202705   \n",
       "7               NaN                         40.202705   \n",
       "8               NaN                         40.202705   \n",
       "9              -2.0                         40.202705   \n",
       "\n",
       "   03LIC_1071_pv_at_action  03LIC_1071_roc_percent  03LIC_1071_roc_direction  \n",
       "0                40.184917               -0.044246                         0  \n",
       "1                40.184917               -0.044246                         0  \n",
       "2                40.184917               -0.044246                         0  \n",
       "3                40.184917               -0.044246                         0  \n",
       "4                38.263474               -4.823633                         0  \n",
       "5                38.263474               -4.823633                         0  \n",
       "6                38.953682               -3.106813                         0  \n",
       "7                38.953682               -3.106813                         0  \n",
       "8                38.953682               -3.106813                         0  \n",
       "9                38.953682               -3.106813                         0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 10: Convert to DataFrame and examine structure\n",
    "context_df = pd.DataFrame(context_records)\n",
    "\n",
    "print(f\"Context DataFrame shape: {context_df.shape}\")\n",
    "print(f\"Total columns: {len(context_df.columns)}\")\n",
    "print(f\"\\nColumn breakdown:\")\n",
    "print(f\"  - Metadata columns: 10 (episode_id, alarm_start, alarm_end, deviation_start, action_timestamp, action_source, action_value, action_prev_value, action_magnitude, action_direction)\")\n",
    "print(f\"  - PV context columns: {len(context_pv_tags) * 4} (4 features x {len(context_pv_tags)} tags)\")\n",
    "\n",
    "# Show first few rows with key columns\n",
    "key_cols = ['episode_id', 'action_timestamp', 'action_source', 'action_direction', 'action_magnitude',\n",
    "            '03LIC_1071_pv_at_deviation_start', '03LIC_1071_pv_at_action', \n",
    "            '03LIC_1071_roc_percent', '03LIC_1071_roc_direction']\n",
    "context_df[key_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6596b04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Context DataFrame Summary ===\n",
      "\n",
      "Actions per episode:\n",
      "  Min: 6, Max: 136, Mean: 33.4\n",
      "\n",
      "Actions by target tag:\n",
      "action_source\n",
      "03LIC_1071    400\n",
      "03PIC_1013     54\n",
      "03LIC_1016     14\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Action direction distribution:\n",
      "action_direction\n",
      "0.0    336\n",
      "1.0     96\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Columns with missing values: 6\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Data quality check and summary statistics\n",
    "print(\"=== Context DataFrame Summary ===\\n\")\n",
    "\n",
    "# Count actions per episode\n",
    "actions_per_episode = context_df.groupby('episode_id').size()\n",
    "print(f\"Actions per episode:\")\n",
    "print(f\"  Min: {actions_per_episode.min()}, Max: {actions_per_episode.max()}, Mean: {actions_per_episode.mean():.1f}\")\n",
    "\n",
    "# Actions by source\n",
    "print(f\"\\nActions by target tag:\")\n",
    "print(context_df['action_source'].value_counts())\n",
    "\n",
    "# Action direction distribution\n",
    "print(f\"\\nAction direction distribution:\")\n",
    "print(context_df['action_direction'].value_counts())\n",
    "\n",
    "# Missing values check\n",
    "missing_cols = context_df.isnull().sum()\n",
    "cols_with_missing = missing_cols[missing_cols > 0]\n",
    "if len(cols_with_missing) > 0:\n",
    "    print(f\"\\nColumns with missing values: {len(cols_with_missing)}\")\n",
    "else:\n",
    "    print(f\"\\nNo missing values in context features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "384991a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before cleaning: 468\n",
      "Total records after removing actions without valid magnitude: 216\n",
      "Total records after removing duplicates: 216\n",
      "\n",
      "Actions per episode after cleaning:\n",
      "  Min: 3, Max: 67, Mean: 15.4\n",
      "\n",
      "Actions by target tag after cleaning:\n",
      "action_source\n",
      "03LIC_1071    185\n",
      "03PIC_1013     26\n",
      "03LIC_1016      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Clean duplicate records (same action appears multiple times with/without PrevValue)\n",
    "# Keep only records with valid action_magnitude (not NaN)\n",
    "print(f\"Total records before cleaning: {len(context_df)}\")\n",
    "\n",
    "# Filter out records where action_magnitude is NaN (these are duplicates without PrevValue)\n",
    "context_df_clean = context_df[context_df['action_magnitude'].notna()].copy()\n",
    "\n",
    "print(f\"Total records after removing actions without valid magnitude: {len(context_df_clean)}\")\n",
    "\n",
    "# Also remove exact duplicates (same timestamp, source, value)\n",
    "context_df_clean = context_df_clean.drop_duplicates(\n",
    "    subset=['episode_id', 'action_timestamp', 'action_source', 'action_value']\n",
    ")\n",
    "\n",
    "print(f\"Total records after removing duplicates: {len(context_df_clean)}\")\n",
    "\n",
    "# Summary after cleaning\n",
    "print(f\"\\nActions per episode after cleaning:\")\n",
    "actions_per_episode_clean = context_df_clean.groupby('episode_id').size()\n",
    "print(f\"  Min: {actions_per_episode_clean.min()}, Max: {actions_per_episode_clean.max()}, Mean: {actions_per_episode_clean.mean():.1f}\")\n",
    "\n",
    "print(f\"\\nActions by target tag after cleaning:\")\n",
    "print(context_df_clean['action_source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "616dc795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context data saved to: /home/h604827/ControlActions/RESULTS/similarity_test_results/similarity_context_training_20episodes.csv\n",
      "Episode split info saved to: RESULTS/similarity_test_results/similarity_approach_episodes_split.json\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Save the context DataFrame for later use\n",
    "output_path = '/home/h604827/ControlActions/RESULTS/similarity_test_results/similarity_context_training_20episodes.csv'\n",
    "context_df_clean.to_csv(output_path, index=False)\n",
    "print(f\"Context data saved to: {output_path}\")\n",
    "\n",
    "# Also save the list of training and testing episode IDs\n",
    "training_testing_info = {\n",
    "    'training_episodes': selected_episodes_df['EpisodeID'].tolist(),\n",
    "    'testing_episodes': remaining_episodes_df['EpisodeID'].tolist()\n",
    "}\n",
    "import json\n",
    "with open('/home/h604827/ControlActions/RESULTS/similarity_test_results/similarity_approach_episodes_split.json', 'w') as f:\n",
    "    json.dump(training_testing_info, f, indent=2)\n",
    "print(\"Episode split info saved to: RESULTS/similarity_test_results/similarity_approach_episodes_split.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "385fe13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Final Context DataFrame ===\n",
      "Shape: (216, 122)\n",
      "Columns: ['03LIC_1071_pv_at_deviation_start', '03LIC_1071_pv_at_action', '03LIC_1071_roc_percent', '03LIC_1071_roc_direction', '02FI_1000_pv_at_deviation_start', '02FI_1000_pv_at_action', '02FI_1000_roc_percent', '02FI_1000_roc_direction', '03FIC_1085_pv_at_deviation_start', '03FIC_1085_pv_at_action', '03FIC_1085_roc_percent', '03FIC_1085_roc_direction', '03FIC_3415_pv_at_deviation_start', '03FIC_3415_pv_at_action', '03FIC_3415_roc_percent', '03FIC_3415_roc_direction', '03FIC_3435_pv_at_deviation_start', '03FIC_3435_pv_at_action', '03FIC_3435_roc_percent', '03FIC_3435_roc_direction']... (and 102 more)\n",
      "\n",
      "ROC columns (28): ['03LIC_1071_roc_percent', '02FI_1000_roc_percent', '03FIC_1085_roc_percent', '03FIC_3415_roc_percent', '03FIC_3435_roc_percent']...\n",
      "Direction columns (28): ['03LIC_1071_roc_direction', '02FI_1000_roc_direction', '03FIC_1085_roc_direction', '03FIC_3415_roc_direction', '03FIC_3435_roc_direction']...\n",
      "\n",
      "=== 03LIC_1071 ROC Statistics ===\n",
      "count    216.000000\n",
      "mean     -13.618427\n",
      "std       55.527318\n",
      "min     -102.930396\n",
      "25%      -52.098870\n",
      "50%      -12.347953\n",
      "75%       14.881934\n",
      "max      197.460189\n",
      "Name: 03LIC_1071_roc_percent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 14: Display sample of the final context dataframe\n",
    "print(\"=== Final Context DataFrame ===\")\n",
    "print(f\"Shape: {context_df_clean.shape}\")\n",
    "print(f\"Columns: {context_df_clean.columns.tolist()[:20]}... (and {len(context_df_clean.columns)-20} more)\")\n",
    "\n",
    "# Show sample with all ROC and direction columns for context overview\n",
    "roc_cols = [c for c in context_df_clean.columns if '_roc_percent' in c]\n",
    "dir_cols = [c for c in context_df_clean.columns if '_roc_direction' in c]\n",
    "\n",
    "print(f\"\\nROC columns ({len(roc_cols)}): {roc_cols[:5]}...\")\n",
    "print(f\"Direction columns ({len(dir_cols)}): {dir_cols[:5]}...\")\n",
    "\n",
    "# Show statistics for target tag 1071\n",
    "print(f\"\\n=== 03LIC_1071 ROC Statistics ===\")\n",
    "print(context_df_clean['03LIC_1071_roc_percent'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cd23e0",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Weighted Multi-Component Similarity Approach\n",
    "\n",
    "### Testing on Reserved Episodes\n",
    "\n",
    "For each test episode:\n",
    "1. Start from deviation start time\n",
    "2. For each minute until alarm_end + 60 minutes:\n",
    "   - Calculate context at that minute\n",
    "   - Compare with all training contexts using weighted similarity\n",
    "   - Return the most similar historical action(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8afc25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC columns: 28\n",
      "Direction columns: 28\n",
      "\n",
      "Tag weights for controllable tags:\n",
      "  03LIC_1071_roc_percent: 3.0\n",
      "  03LIC_1016_roc_percent: 2.0\n",
      "  03PIC_1013_roc_percent: 2.0\n"
     ]
    }
   ],
   "source": [
    "# Step 15: Define the Weighted Multi-Component Similarity Function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Get column names for ROC and direction features\n",
    "roc_cols = [c for c in context_df_clean.columns if '_roc_percent' in c]\n",
    "dir_cols = [c for c in context_df_clean.columns if '_roc_direction' in c]\n",
    "pv_action_cols = [c for c in context_df_clean.columns if '_pv_at_action' in c]\n",
    "\n",
    "# Define tag weights (higher weight for target and controllable tags)\n",
    "tag_weights = {}\n",
    "for col in roc_cols:\n",
    "    tag_name = col.replace('_roc_percent', '')\n",
    "    if '1071' in tag_name:\n",
    "        tag_weights[col] = 3.0  # Target tag - highest weight\n",
    "    elif '1016' in tag_name or '1013' in tag_name:\n",
    "        tag_weights[col] = 2.0  # Other controllable targets\n",
    "    else:\n",
    "        tag_weights[col] = 1.0  # Related tags\n",
    "\n",
    "print(f\"ROC columns: {len(roc_cols)}\")\n",
    "print(f\"Direction columns: {len(dir_cols)}\")\n",
    "print(f\"\\nTag weights for controllable tags:\")\n",
    "for k, v in tag_weights.items():\n",
    "    if v > 1:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b0af62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PV action columns for similarity: 28 tags\n",
      "Example columns: ['03LIC_1071_pv_at_action', '02FI_1000_pv_at_action', '03FIC_1085_pv_at_action']\n",
      "\n",
      "Similarity function defined successfully (with cosine-based PV similarity component)\n"
     ]
    }
   ],
   "source": [
    "# Step 16: Implement the weighted similarity calculation function (with PV similarity)\n",
    "\n",
    "def calculate_weighted_similarity(runtime_context, historical_df, \n",
    "                                   roc_cols, dir_cols, pv_action_cols, tag_weights,\n",
    "                                   w_roc=0.5, w_dir=0.35, w_pv=0.15):\n",
    "    \"\"\"\n",
    "    Calculate weighted multi-component similarity between runtime context and all historical contexts.\n",
    "    \n",
    "    Components:\n",
    "    1. ROC Pattern Similarity (Cosine) - weighted by tag importance\n",
    "    2. Direction Match Score (Jaccard-like)\n",
    "    3. PV State Similarity (Cosine similarity on normalized PV values)\n",
    "    \n",
    "    Returns: DataFrame with similarity scores and corresponding action details\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    \n",
    "    # Create weight array for ROC columns\n",
    "    weight_array = np.array([tag_weights.get(col, 1.0) for col in roc_cols])\n",
    "    \n",
    "    # Runtime ROC values (weighted) - handle NaN by replacing with 0\n",
    "    runtime_roc_raw = np.array([runtime_context.get(col, 0) for col in roc_cols])\n",
    "    runtime_roc_raw = np.nan_to_num(runtime_roc_raw, nan=0.0)\n",
    "    runtime_roc = runtime_roc_raw * weight_array\n",
    "    \n",
    "    runtime_dir = np.array([runtime_context.get(col.replace('_roc_percent', '_roc_direction'), 0) for col in roc_cols])\n",
    "    runtime_dir = np.nan_to_num(runtime_dir, nan=0.0)\n",
    "    \n",
    "    # Runtime PV values at action time (for PV state similarity)\n",
    "    runtime_pv = np.array([runtime_context.get(col, 0) for col in pv_action_cols])\n",
    "    runtime_pv = np.nan_to_num(runtime_pv, nan=0.0)\n",
    "    \n",
    "    # Get PV weights (same mapping as ROC weights)\n",
    "    pv_weight_array = np.array([tag_weights.get(col.replace('_pv_at_action', '_roc_percent'), 1.0) \n",
    "                                for col in pv_action_cols])\n",
    "    \n",
    "    for idx, hist_row in historical_df.iterrows():\n",
    "        # 1. Cosine Similarity on weighted ROC pattern\n",
    "        hist_roc_raw = np.array([hist_row[col] if pd.notna(hist_row[col]) else 0 for col in roc_cols])\n",
    "        hist_roc = hist_roc_raw * weight_array\n",
    "        \n",
    "        # Handle zero vectors\n",
    "        runtime_norm = np.linalg.norm(runtime_roc)\n",
    "        hist_norm = np.linalg.norm(hist_roc)\n",
    "        \n",
    "        if runtime_norm == 0 or hist_norm == 0:\n",
    "            roc_sim = 0.5  # Default to neutral similarity\n",
    "        else:\n",
    "            # Manual cosine similarity to avoid sklearn NaN issues\n",
    "            dot_product = np.dot(runtime_roc, hist_roc)\n",
    "            roc_sim = dot_product / (runtime_norm * hist_norm)\n",
    "            # Normalize from [-1, 1] to [0, 1]\n",
    "            roc_sim = (roc_sim + 1) / 2\n",
    "        \n",
    "        # 2. Direction Match Score\n",
    "        hist_dir = np.array([hist_row[col.replace('_roc_percent', '_roc_direction')] \n",
    "                           if pd.notna(hist_row[col.replace('_roc_percent', '_roc_direction')]) else 0 \n",
    "                           for col in roc_cols])\n",
    "        dir_match = np.mean(runtime_dir == hist_dir)\n",
    "        \n",
    "        # 3. PV State Similarity using Cosine Similarity (scale-invariant)\n",
    "        hist_pv = np.array([hist_row[col] if pd.notna(hist_row[col]) else 0 for col in pv_action_cols])\n",
    "        \n",
    "        # Apply tag weights to PV values\n",
    "        weighted_runtime_pv = runtime_pv * pv_weight_array\n",
    "        weighted_hist_pv = hist_pv * pv_weight_array\n",
    "        \n",
    "        # Cosine similarity for PV values (handles different scales naturally)\n",
    "        runtime_pv_norm = np.linalg.norm(weighted_runtime_pv)\n",
    "        hist_pv_norm = np.linalg.norm(weighted_hist_pv)\n",
    "        \n",
    "        if runtime_pv_norm == 0 or hist_pv_norm == 0:\n",
    "            pv_sim = 0.5  # Default to neutral similarity\n",
    "        else:\n",
    "            pv_dot = np.dot(weighted_runtime_pv, weighted_hist_pv)\n",
    "            pv_sim = pv_dot / (runtime_pv_norm * hist_pv_norm)\n",
    "            # Normalize from [-1, 1] to [0, 1]\n",
    "            pv_sim = (pv_sim + 1) / 2\n",
    "        \n",
    "        # 4. Combined weighted similarity\n",
    "        total_similarity = w_roc * roc_sim + w_dir * dir_match + w_pv * pv_sim\n",
    "        \n",
    "        similarities.append({\n",
    "            'hist_index': idx,\n",
    "            'total_similarity': total_similarity,\n",
    "            'roc_similarity': roc_sim,\n",
    "            'direction_match': dir_match,\n",
    "            'pv_similarity': pv_sim,\n",
    "            'action_source': hist_row['action_source'],\n",
    "            'action_direction': hist_row['action_direction'],\n",
    "            'action_magnitude': hist_row['action_magnitude'],\n",
    "            'episode_id': hist_row['episode_id']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(similarities).sort_values('total_similarity', ascending=False)\n",
    "\n",
    "# Get the pv_at_action columns for the similarity function\n",
    "pv_action_cols = [col for col in context_df_clean.columns if col.endswith('_pv_at_action')]\n",
    "print(f\"PV action columns for similarity: {len(pv_action_cols)} tags\")\n",
    "print(f\"Example columns: {pv_action_cols[:3]}\")\n",
    "\n",
    "print(\"\\nSimilarity function defined successfully (with cosine-based PV similarity component)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "935bbd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime context builder function defined\n"
     ]
    }
   ],
   "source": [
    "# Step 17: Function to build context at a specific timestamp for runtime evaluation\n",
    "\n",
    "def build_runtime_context(deviation_start, current_time, pv_tags):\n",
    "    \"\"\"\n",
    "    Build context at runtime - from deviation_start to current_time.\n",
    "    Same structure as training context but calculated at runtime.\n",
    "    \"\"\"\n",
    "    context = {}\n",
    "    \n",
    "    for pv_tag in pv_tags:\n",
    "        tag_base = pv_tag.replace('.PV', '')\n",
    "        \n",
    "        # Get PV at deviation start\n",
    "        pv_at_start = get_pv_at_timestamp(deviation_start, pv_tag)\n",
    "        \n",
    "        # Get PV at current time\n",
    "        pv_at_current = get_pv_at_timestamp(current_time, pv_tag)\n",
    "        \n",
    "        # Calculate ROC percent\n",
    "        if pv_at_start is not None and pv_at_start != 0 and not np.isnan(pv_at_start):\n",
    "            roc_percent = ((pv_at_current - pv_at_start) / abs(pv_at_start)) * 100\n",
    "        else:\n",
    "            roc_percent = 0.0\n",
    "        \n",
    "        # ROC direction (1 = positive/rising, 0 = negative/falling)\n",
    "        roc_direction = 1 if roc_percent >= 0 else 0\n",
    "        \n",
    "        context[f'{tag_base}_pv_at_deviation_start'] = pv_at_start\n",
    "        context[f'{tag_base}_pv_at_action'] = pv_at_current\n",
    "        context[f'{tag_base}_roc_percent'] = roc_percent\n",
    "        context[f'{tag_base}_roc_direction'] = roc_direction\n",
    "    \n",
    "    return context\n",
    "\n",
    "print(\"Runtime context builder function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6 test episodes...\n",
      "Output directory: /home/h604827/ControlActions/RESULTS/similarity_test_results\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing Episode 388...\n",
      "============================================================\n",
      "  Alarm: 2024-06-09 14:40:00 to 2024-06-09 14:44:00\n",
      "  Deviation Start: 2024-06-09 13:14:00\n",
      "  Actual Operated Tags: 03PIC_1013\n",
      "  Total minutes to evaluate: 151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 388:   0%|          | 0/151 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 388: 100%|██████████| 151/151 [00:09<00:00, 15.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_388_minute_results.csv\n",
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_388_visualization.html\n",
      "  Actual actions: 3\n",
      "  Tag match accuracy: 0/3 = 0.0%\n",
      "\n",
      "============================================================\n",
      "Processing Episode 400...\n",
      "============================================================\n",
      "  Alarm: 2024-06-17 15:46:00 to 2024-06-17 16:28:00\n",
      "  Deviation Start: 2024-06-17 14:20:00\n",
      "  Actual Operated Tags: 03PIC_1013\n",
      "  Total minutes to evaluate: 189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 400: 100%|██████████| 189/189 [00:11<00:00, 15.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_400_minute_results.csv\n",
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_400_visualization.html\n",
      "  Actual actions: 21\n",
      "  Tag match accuracy: 10/21 = 47.6%\n",
      "\n",
      "============================================================\n",
      "Processing Episode 469...\n",
      "============================================================\n",
      "  Alarm: 2024-09-21 08:07:00 to 2024-09-21 08:24:00\n",
      "  Deviation Start: 2024-09-21 06:41:00\n",
      "  Actual Operated Tags: 03LIC_1071\n",
      "  Total minutes to evaluate: 164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 469: 100%|██████████| 164/164 [00:10<00:00, 16.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_469_minute_results.csv\n",
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_469_visualization.html\n",
      "  Actual actions: 57\n",
      "  Tag match accuracy: 50/57 = 87.7%\n",
      "\n",
      "============================================================\n",
      "Processing Episode 521...\n",
      "============================================================\n",
      "  Alarm: 2025-01-05 07:16:00 to 2025-01-05 08:06:00\n",
      "  Deviation Start: 2025-01-05 05:51:00\n",
      "  Actual Operated Tags: 03LIC_1071\n",
      "  Total minutes to evaluate: 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 521: 100%|██████████| 196/196 [00:12<00:00, 15.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_521_minute_results.csv\n",
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_521_visualization.html\n",
      "  Actual actions: 1\n",
      "  Tag match accuracy: 1/1 = 100.0%\n",
      "\n",
      "============================================================\n",
      "Processing Episode 522...\n",
      "============================================================\n",
      "  Alarm: 2025-01-05 08:24:00 to 2025-01-05 08:31:00\n",
      "  Deviation Start: 2025-01-05 06:58:00\n",
      "  Actual Operated Tags: 03LIC_1071\n",
      "  Total minutes to evaluate: 154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 522: 100%|██████████| 154/154 [00:09<00:00, 16.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_522_minute_results.csv\n",
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_522_visualization.html\n",
      "  Actual actions: 1\n",
      "  Tag match accuracy: 1/1 = 100.0%\n",
      "\n",
      "============================================================\n",
      "Processing Episode 604...\n",
      "============================================================\n",
      "  Alarm: 2025-06-21 22:15:00 to 2025-06-21 22:16:00\n",
      "  Deviation Start: 2025-06-21 20:49:00\n",
      "  Actual Operated Tags: 03LIC_1071\n",
      "  Total minutes to evaluate: 148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 604: 100%|██████████| 148/148 [00:09<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_604_minute_results.csv\n",
      "  Saved: /home/h604827/ControlActions/RESULTS/similarity_test_results/episode_604_visualization.html\n",
      "  Actual actions: 1\n",
      "  Tag match accuracy: 1/1 = 100.0%\n",
      "\n",
      "============================================================\n",
      "ALL EPISODES PROCESSED SUCCESSFULLY!\n",
      "============================================================\n",
      "\n",
      "Results saved to: /home/h604827/ControlActions/RESULTS/similarity_test_results/\n",
      "  - 6 episode CSV files (minute-by-minute results)\n",
      "  - 6 episode HTML visualizations\n",
      "  - all_episodes_summary.csv\n",
      "  - all_episodes_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Step 25: Run similarity approach on all 7 test episodes and save results\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '/home/h604827/ControlActions/RESULTS/similarity_test_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Test episodes (remaining 6, excluding 386 which we already tested)\n",
    "test_episode_ids = [386, 388, 400, 469, 521, 522, 604]\n",
    "\n",
    "# Store summary for all episodes\n",
    "all_episodes_summary = []\n",
    "\n",
    "print(f\"Processing {len(test_episode_ids)} test episodes...\")\n",
    "print(f\"Output directory: {output_dir}\\n\")\n",
    "\n",
    "for test_ep_id in test_episode_ids:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing Episode {test_ep_id}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get episode details\n",
    "    ep_data = episodes_operated_tags_df[episodes_operated_tags_df['EpisodeID'] == test_ep_id].iloc[0]\n",
    "    ep_alarm_start = ep_data['AlarmStart']\n",
    "    ep_alarm_end = ep_data['AlarmEnd']\n",
    "    ep_operated_tags = ep_data['OperatedTags']\n",
    "    \n",
    "    # Get deviation start\n",
    "    ep_deviation_start = get_deviation_start_for_episode(test_ep_id, ep_alarm_start, ep_alarm_end)\n",
    "    \n",
    "    # Calculate time range\n",
    "    ep_end_time = ep_alarm_end + pd.Timedelta(minutes=60)\n",
    "    ep_total_minutes = int((ep_end_time - ep_deviation_start).total_seconds() / 60) + 1\n",
    "    \n",
    "    print(f\"  Alarm: {ep_alarm_start} to {ep_alarm_end}\")\n",
    "    print(f\"  Deviation Start: {ep_deviation_start}\")\n",
    "    print(f\"  Actual Operated Tags: {ep_operated_tags}\")\n",
    "    print(f\"  Total minutes to evaluate: {ep_total_minutes}\")\n",
    "    \n",
    "    # Run similarity matching for each minute\n",
    "    ep_minute_results = []\n",
    "    \n",
    "    for minute_offset in tqdm(range(ep_total_minutes), desc=f\"Episode {test_ep_id}\"):\n",
    "        current_time = ep_deviation_start + pd.Timedelta(minutes=minute_offset)\n",
    "        \n",
    "        # Build runtime context\n",
    "        runtime_ctx = build_runtime_context(ep_deviation_start, current_time, context_pv_tags)\n",
    "        \n",
    "        # Calculate similarity (including PV state similarity)\n",
    "        sim_results = calculate_weighted_similarity(\n",
    "            runtime_ctx, context_df_clean, roc_cols, dir_cols, pv_action_cols, tag_weights\n",
    "        )\n",
    "        \n",
    "        # Get top 3 matches\n",
    "        for rank, (_, match) in enumerate(sim_results.head(3).iterrows(), 1):\n",
    "            ep_minute_results.append({\n",
    "                'episode_id': test_ep_id,\n",
    "                'minute_offset': minute_offset,\n",
    "                'current_time': current_time,\n",
    "                'rank': rank,\n",
    "                'similarity': match['total_similarity'],\n",
    "                'roc_similarity': match['roc_similarity'],\n",
    "                'direction_match': match['direction_match'],\n",
    "                'pv_similarity': match['pv_similarity'],  # Added PV similarity\n",
    "                'recommended_action_source': match['action_source'],\n",
    "                'recommended_action_direction': match['action_direction'],\n",
    "                'recommended_action_magnitude': match['action_magnitude'],\n",
    "                'matched_episode_id': match['episode_id'],\n",
    "                '1071_roc_percent': runtime_ctx['03LIC_1071_roc_percent'],\n",
    "                '1071_roc_direction': runtime_ctx['03LIC_1071_roc_direction']\n",
    "            })\n",
    "    \n",
    "    ep_results_df = pd.DataFrame(ep_minute_results)\n",
    "    ep_top_recs = ep_results_df[ep_results_df['rank'] == 1].copy()\n",
    "    \n",
    "    # Get actual actions for this episode\n",
    "    ep_actual_actions, _ = get_operator_actions_for_episode(\n",
    "        ep_alarm_start, ep_alarm_end,\n",
    "        target_sources=['03LIC_1071', '03LIC_1016', '03PIC_1013']\n",
    "    )\n",
    "    \n",
    "    if len(ep_actual_actions) > 0:\n",
    "        ep_actual_clean = ep_actual_actions.dropna(subset=['PrevValue'])\n",
    "        ep_actual_clean = ep_actual_clean.drop_duplicates(subset=['VT_Start', 'Source', 'Value'])\n",
    "        ep_actual_clean = ep_actual_clean.copy()\n",
    "        \n",
    "        # Handle non-numeric values safely\n",
    "        try:\n",
    "            ep_actual_clean['action_magnitude'] = pd.to_numeric(ep_actual_clean['Value'], errors='coerce') - pd.to_numeric(ep_actual_clean['PrevValue'], errors='coerce')\n",
    "            ep_actual_clean['action_direction'] = (ep_actual_clean['action_magnitude'] > 0).astype(int)\n",
    "            # Remove rows where we couldn't compute magnitude\n",
    "            ep_actual_clean = ep_actual_clean.dropna(subset=['action_magnitude'])\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not compute action magnitude: {e}\")\n",
    "            ep_actual_clean = pd.DataFrame()\n",
    "    else:\n",
    "        ep_actual_clean = pd.DataFrame()\n",
    "    \n",
    "    # Save minute-by-minute results CSV\n",
    "    csv_path = f'{output_dir}/episode_{test_ep_id}_minute_results.csv'\n",
    "    ep_results_df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Create and save visualization\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=[\n",
    "            f'Episode {test_ep_id}: 03LIC_1071.PV Trend',\n",
    "            'Recommended Action Source Over Time',\n",
    "            'Similarity Score & 1071 ROC% Over Time'\n",
    "        ],\n",
    "        vertical_spacing=0.1,\n",
    "        row_heights=[0.35, 0.3, 0.35]\n",
    "    )\n",
    "    \n",
    "    # Get PV data\n",
    "    pv_start = ep_deviation_start - pd.Timedelta(minutes=10)\n",
    "    pv_end = ep_end_time + pd.Timedelta(minutes=10)\n",
    "    pv_window = pv_op_data_df.loc[pv_start:pv_end, '03LIC_1071.PV']\n",
    "    \n",
    "    # Row 1: PV Trend\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=pv_window.index, y=pv_window.values, mode='lines', name='03LIC_1071.PV', line=dict(color='blue')),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_hline(y=28.75, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Alarm Threshold\", row=1, col=1)\n",
    "    fig.add_vrect(x0=ep_deviation_start, x1=ep_alarm_start, fillcolor=\"orange\", opacity=0.1, line_width=0, row=1, col=1)\n",
    "    fig.add_vrect(x0=ep_alarm_start, x1=ep_alarm_end, fillcolor=\"red\", opacity=0.2, line_width=0, row=1, col=1)\n",
    "    \n",
    "    # Add actual action markers\n",
    "    if len(ep_actual_clean) > 0:\n",
    "        action_times = pd.to_datetime(ep_actual_clean['VT_Start'])\n",
    "        action_pv_vals = [get_pv_at_timestamp(t, '03LIC_1071.PV') for t in action_times]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=action_times, y=action_pv_vals, mode='markers', name=f'Actual: {ep_operated_tags}',\n",
    "                      marker=dict(symbol='triangle-up', size=12, color='red')),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Row 2: Recommended action source\n",
    "    source_map = {'03LIC_1071': 0, '03LIC_1016': 1, '03PIC_1013': 2}\n",
    "    ep_top_recs['source_numeric'] = ep_top_recs['recommended_action_source'].map(source_map)\n",
    "    colors = ['green' if d == 1 else 'red' for d in ep_top_recs['recommended_action_direction']]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ep_top_recs['current_time'], y=ep_top_recs['source_numeric'],\n",
    "                  mode='markers', marker=dict(size=5, color=colors),\n",
    "                  name='Recommended (green=↑, red=↓)',\n",
    "                  text=[f\"Tag: {s}<br>Dir: {'↑' if d==1 else '↓'}<br>Sim: {sim:.3f}\" \n",
    "                        for s, d, sim in zip(ep_top_recs['recommended_action_source'],\n",
    "                                             ep_top_recs['recommended_action_direction'],\n",
    "                                             ep_top_recs['similarity'])],\n",
    "                  hoverinfo='text'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_vrect(x0=ep_alarm_start, x1=ep_alarm_end, fillcolor=\"red\", opacity=0.2, line_width=0, row=2, col=1)\n",
    "    \n",
    "    # Row 3: Similarity and ROC\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ep_top_recs['current_time'], y=ep_top_recs['similarity'],\n",
    "                  mode='lines', name='Similarity', line=dict(color='purple')),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=ep_top_recs['current_time'], y=ep_top_recs['1071_roc_percent'],\n",
    "                  mode='lines', name='1071 ROC%', line=dict(color='orange')),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    fig.add_vrect(x0=ep_alarm_start, x1=ep_alarm_end, fillcolor=\"red\", opacity=0.2, line_width=0, row=3, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=900,\n",
    "        title_text=f'Episode {test_ep_id} - Similarity-Based Recommendations<br>'\n",
    "                   f'<sub>Actual: {ep_operated_tags} | Alarm: {ep_alarm_start} to {ep_alarm_end}</sub>',\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Level\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Tag\", ticktext=['1071', '1016', '1013'], tickvals=[0, 1, 2], row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score / ROC%\", row=3, col=1)\n",
    "    \n",
    "    # Save visualization\n",
    "    html_path = f'{output_dir}/episode_{test_ep_id}_visualization.html'\n",
    "    fig.write_html(html_path)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    rec_source_dist = ep_top_recs['recommended_action_source'].value_counts().to_dict()\n",
    "    rec_dir_dist = ep_top_recs['recommended_action_direction'].value_counts().to_dict()\n",
    "    \n",
    "    # Accuracy at actual action times\n",
    "    tag_matches = 0\n",
    "    dir_matches = 0\n",
    "    total_actual = len(ep_actual_clean) if len(ep_actual_clean) > 0 else 0\n",
    "    \n",
    "    if total_actual > 0:\n",
    "        for _, act in ep_actual_clean.iterrows():\n",
    "            act_time = pd.to_datetime(act['VT_Start'])\n",
    "            time_diff = abs((ep_top_recs['current_time'] - act_time).dt.total_seconds())\n",
    "            closest_idx = time_diff.idxmin()\n",
    "            closest_rec = ep_top_recs.loc[closest_idx]\n",
    "            \n",
    "            if act['Source'] == closest_rec['recommended_action_source']:\n",
    "                tag_matches += 1\n",
    "            act_dir_val = 1 if act['action_direction'] == 1 else 0\n",
    "            if act_dir_val == closest_rec['recommended_action_direction']:\n",
    "                dir_matches += 1\n",
    "    \n",
    "    summary = {\n",
    "        'episode_id': test_ep_id,\n",
    "        'alarm_start': str(ep_alarm_start),\n",
    "        'alarm_end': str(ep_alarm_end),\n",
    "        'deviation_start': str(ep_deviation_start),\n",
    "        'actual_operated_tags': ep_operated_tags,\n",
    "        'total_minutes_evaluated': ep_total_minutes,\n",
    "        'actual_actions_count': total_actual,\n",
    "        'tag_match_accuracy': tag_matches / total_actual if total_actual > 0 else None,\n",
    "        'direction_match_accuracy': dir_matches / total_actual if total_actual > 0 else None,\n",
    "        'recommended_source_distribution': rec_source_dist,\n",
    "        'recommended_direction_distribution': rec_dir_dist\n",
    "    }\n",
    "    all_episodes_summary.append(summary)\n",
    "    \n",
    "    print(f\"  Saved: {csv_path}\")\n",
    "    print(f\"  Saved: {html_path}\")\n",
    "    print(f\"  Actual actions: {total_actual}\")\n",
    "    if total_actual > 0:\n",
    "        print(f\"  Tag match accuracy: {tag_matches}/{total_actual} = {tag_matches/total_actual*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"  No actual actions with valid magnitude\")\n",
    "\n",
    "# Save overall summary\n",
    "summary_df = pd.DataFrame(all_episodes_summary)\n",
    "summary_path = f'{output_dir}/all_episodes_summary.csv'\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "# Also save as JSON for detailed info\n",
    "import json\n",
    "with open(f'{output_dir}/all_episodes_summary.json', 'w') as f:\n",
    "    json.dump(all_episodes_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"ALL EPISODES PROCESSED SUCCESSFULLY!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nResults saved to: {output_dir}/\")\n",
    "print(f\"  - 6 episode CSV files (minute-by-minute results)\")\n",
    "print(f\"  - 6 episode HTML visualizations\")\n",
    "print(f\"  - all_episodes_summary.csv\")\n",
    "print(f\"  - all_episodes_summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2218914d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 386 results saved to: /home/h604827/ControlActions/RESULTS/similarity_test_results/\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY - ALL 7 TEST EPISODES\n",
      "============================================================\n",
      "\n",
      "Results for all 7 test episodes:\n",
      " episode_id actual_operated_tags  actual_actions_count  tag_match_accuracy\n",
      "        386           03LIC_1071                    58            0.879310\n",
      "        388           03PIC_1013                     3            0.000000\n",
      "        400           03PIC_1013                    21            0.476190\n",
      "        469           03LIC_1071                    57            0.877193\n",
      "        521           03LIC_1071                     1            1.000000\n",
      "        522           03LIC_1071                     1            1.000000\n",
      "        604           03LIC_1071                     1            1.000000\n",
      "\n",
      "All results saved to: /home/h604827/ControlActions/RESULTS/similarity_test_results/\n"
     ]
    }
   ],
   "source": [
    "# Combine all summaries\n",
    "all_7_summaries = all_episodes_summary\n",
    "final_summary_df = pd.DataFrame(all_7_summaries)\n",
    "final_summary_df.to_csv(f'{output_dir}/all_7_episodes_summary.csv', index=False)\n",
    "\n",
    "# Save as JSON\n",
    "with open(f'{output_dir}/all_7_episodes_summary.json', 'w') as f:\n",
    "    json.dump(all_7_summaries, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Episode 386 results saved to: {output_dir}/\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL SUMMARY - ALL 7 TEST EPISODES\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nResults for all 7 test episodes:\")\n",
    "print(final_summary_df[['episode_id', 'actual_operated_tags', 'actual_actions_count', 'tag_match_accuracy']].to_string(index=False))\n",
    "print(f\"\\nAll results saved to: {output_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305e03dc",
   "metadata": {},
   "source": [
    "---\n",
    "## Operator Action vs Recommendation Report (Per Episode)\n",
    "\n",
    "This section exports a multi-sheet Excel file. Each sheet corresponds to one episode and lists each actual operator action followed by its top-3 recommended actions at the closest recommendation time, with matching flags and key fields for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2ad663ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-episode operator action report saved to: /home/h604827/ControlActions/RESULTS/similarity_test_results/operator_action_recommendations_by_episode.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Build per-episode Excel report comparing actual actions vs top-3 recommendations\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "output_dir = '/home/h604827/ControlActions/RESULTS/similarity_test_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Episodes to include (7 test episodes)\n",
    "episode_ids = [386, 388, 400, 469, 521, 522, 604]\n",
    "events_df = events_df[events_df['ConditionName'] == 'CHANGE'].copy()\n",
    "\n",
    "def load_minute_results(episode_id):\n",
    "    \"\"\"Load minute-by-minute similarity results for an episode from disk.\"\"\"\n",
    "    csv_path = f\"{output_dir}/episode_{episode_id}_minute_results.csv\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Missing minute results CSV: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['current_time'] = pd.to_datetime(df['current_time'])\n",
    "    return df\n",
    "\n",
    "def clean_actual_actions(actions_df):\n",
    "    \"\"\"Keep valid operator actions and compute magnitude/direction.\"\"\"\n",
    "    if len(actions_df) == 0:\n",
    "        return actions_df.copy()\n",
    "    cleaned = actions_df.dropna(subset=['PrevValue']).drop_duplicates(subset=['VT_Start', 'Source', 'Value']).copy()\n",
    "    cleaned['action_magnitude'] = pd.to_numeric(cleaned['Value'], errors='coerce') - pd.to_numeric(cleaned['PrevValue'], errors='coerce')\n",
    "    cleaned['action_direction'] = (cleaned['action_magnitude'] > 0).astype(int)\n",
    "    cleaned = cleaned.dropna(subset=['action_magnitude'])\n",
    "    return cleaned\n",
    "\n",
    "# Build a fast lookup for action type (Description) by Source + VT_Start\n",
    "if events_df['VT_Start'].dtype == object:\n",
    "    events_df['VT_Start'] = pd.to_datetime(events_df['VT_Start'])\n",
    "\n",
    "desc_lookup = (\n",
    "    events_df.dropna(subset=['Description'])\n",
    "    .drop_duplicates(subset=['Source', 'VT_Start'])\n",
    "    .set_index(['Source', 'VT_Start'])['Description']\n",
    " )\n",
    "\n",
    "def normalize_action_type(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    return str(val).strip()\n",
    "\n",
    "def get_action_type(source, ts):\n",
    "    key = (source, ts)\n",
    "    try:\n",
    "        return normalize_action_type(desc_lookup.loc[key])\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def find_recommended_action_type(rec):\n",
    "    \"\"\"Find action type for a recommended action using historical context + events lookup.\"\"\"\n",
    "    rec_source = rec['recommended_action_source']\n",
    "    rec_dir = int(rec['recommended_action_direction'])\n",
    "    rec_mag = rec['recommended_action_magnitude']\n",
    "    if pd.isna(rec_mag):\n",
    "        return None\n",
    "    hist_match = context_df_clean[\n",
    "        (context_df_clean['episode_id'] == rec['matched_episode_id']) &\n",
    "        (context_df_clean['action_source'] == rec_source) &\n",
    "        (context_df_clean['action_direction'] == rec_dir) &\n",
    "        (np.isclose(context_df_clean['action_magnitude'], rec_mag, atol=1e-6))\n",
    "    ]\n",
    "    if len(hist_match) == 0:\n",
    "        return None\n",
    "    hist_ts = pd.to_datetime(hist_match.iloc[0]['action_timestamp'])\n",
    "    return get_action_type(rec_source, hist_ts)\n",
    "\n",
    "def build_episode_rows(episode_id):\n",
    "    \"\"\"Return a DataFrame with actual actions and top-3 recommendations beneath each one.\"\"\"\n",
    "    ep_row = episodes_operated_tags_df[episodes_operated_tags_df['EpisodeID'] == episode_id].iloc[0]\n",
    "    alarm_start = ep_row['AlarmStart']\n",
    "    alarm_end = ep_row['AlarmEnd']\n",
    "\n",
    "    actual_actions, _ = get_operator_actions_for_episode(\n",
    "        alarm_start, alarm_end, target_sources=['03LIC_1071', '03LIC_1016', '03PIC_1013']\n",
    "    )\n",
    "    actual_actions = clean_actual_actions(actual_actions)\n",
    "    actual_actions = actual_actions.sort_values('VT_Start')\n",
    "\n",
    "    minute_df = load_minute_results(episode_id)\n",
    "    top3_df = minute_df[minute_df['rank'].isin([1, 2, 3])].copy()\n",
    "\n",
    "    rows = []\n",
    "    if len(actual_actions) == 0:\n",
    "        rows.append({\n",
    "            'episode_id': episode_id,\n",
    "            'row_type': 'note',\n",
    "            'note': 'No actual actions with valid magnitude in this episode.'\n",
    "        })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    for i, (_, act) in enumerate(actual_actions.iterrows(), start=1):\n",
    "        act_time = pd.to_datetime(act['VT_Start'])\n",
    "        act_source = act['Source']\n",
    "        act_dir = int(act['action_direction'])\n",
    "        act_mag = float(act['action_magnitude'])\n",
    "        act_val = act['Value']\n",
    "        act_prev = act['PrevValue']\n",
    "        act_type = normalize_action_type(act.get('Description', None))\n",
    "\n",
    "        # Actual action row\n",
    "        rows.append({\n",
    "            'episode_id': episode_id,\n",
    "            'action_group': i,\n",
    "            'row_type': 'actual',\n",
    "            'actual_action_time': act_time,\n",
    "            'action_time': act_time,\n",
    "            'source': act_source,\n",
    "            'direction': act_dir,\n",
    "            'magnitude': act_mag,\n",
    "            'value': act_val,\n",
    "            'prev_value': act_prev,\n",
    "            'action_type': act_type,\n",
    "            'similarity': np.nan,\n",
    "            'roc_similarity': np.nan,\n",
    "            'direction_match': np.nan,\n",
    "            'pv_similarity': np.nan,\n",
    "            'matched_episode_id': np.nan,\n",
    "            'time_delta_minutes': 0.0,\n",
    "            'actual_source': act_source,\n",
    "            'actual_direction': act_dir,\n",
    "            'actual_action_type': act_type,\n",
    "            'tag_match_to_actual': np.nan,\n",
    "            'direction_match_to_actual': np.nan,\n",
    "            'action_type_match_to_actual': np.nan\n",
    "        })\n",
    "\n",
    "        # Closest recommendation time (from minute-by-minute results)\n",
    "        time_diff = (top3_df['current_time'] - act_time).abs().dt.total_seconds()\n",
    "        closest_idx = time_diff.idxmin()\n",
    "        closest_time = top3_df.loc[closest_idx, 'current_time']\n",
    "        closest_delta = float((closest_time - act_time).total_seconds() / 60.0)\n",
    "        closest_recs = top3_df[top3_df['current_time'] == closest_time].sort_values('rank')\n",
    "\n",
    "        # Recommended action rows\n",
    "        for _, rec in closest_recs.iterrows():\n",
    "            rec_source = rec['recommended_action_source']\n",
    "            rec_dir = int(rec['recommended_action_direction'])\n",
    "            rec_type = find_recommended_action_type(rec)\n",
    "            rows.append({\n",
    "                'episode_id': episode_id,\n",
    "                'action_group': i,\n",
    "                'row_type': 'recommended',\n",
    "                'actual_action_time': act_time,\n",
    "                'action_time': closest_time,\n",
    "                'source': rec_source,\n",
    "                'direction': rec_dir,\n",
    "                'magnitude': rec['recommended_action_magnitude'],\n",
    "                'value': np.nan,\n",
    "                'prev_value': np.nan,\n",
    "                'action_type': rec_type,\n",
    "                'similarity': rec['similarity'],\n",
    "                'roc_similarity': rec['roc_similarity'],\n",
    "                'direction_match': rec['direction_match'],\n",
    "                'pv_similarity': rec['pv_similarity'],\n",
    "                'matched_episode_id': rec['matched_episode_id'],\n",
    "                'time_delta_minutes': closest_delta,\n",
    "                'actual_source': act_source,\n",
    "                'actual_direction': act_dir,\n",
    "                'actual_action_type': act_type,\n",
    "                'tag_match_to_actual': 1 if rec_source == act_source else 0,\n",
    "                'direction_match_to_actual': 1 if rec_dir == act_dir else 0,\n",
    "                'action_type_match_to_actual': 1 if (rec_type is not None and act_type is not None and rec_type == act_type) else 0\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Build and write Excel with one sheet per episode\n",
    "excel_path = f\"{output_dir}/operator_action_recommendations_by_episode.xlsx\"\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    for ep_id in episode_ids:\n",
    "        sheet_df = build_episode_rows(ep_id)\n",
    "        sheet_name = f\"episode_{ep_id}\"\n",
    "        sheet_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(f\"Per-episode operator action report saved to: {excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b6f105b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tag + Direction + Magnitude + Action Type Match (per episode) ===\n",
      " episode_id  actual_actions  matched_actions  match_pct\n",
      "        386               6                5  83.333333\n",
      "        388               3                0   0.000000\n",
      "        400              21                7  33.333333\n",
      "        469              57               46  80.701754\n",
      "        521               1                0   0.000000\n",
      "        522               1                0   0.000000\n",
      "        604               1                0   0.000000\n",
      "\n",
      "=== Overall Match Across All Episodes ===\n",
      "Total actual actions: 90\n",
      "Matched actions: 58\n",
      "Overall match % (tol=1): 64.44%\n"
     ]
    }
   ],
   "source": [
    "# Episode-level and overall match stats (tag + direction + magnitude + action type)\n",
    "magnitude_tolerance = 1  # abs(diff) <= this value counts as match\n",
    "\n",
    "# Ensure Description lookup is available\n",
    "if events_df['VT_Start'].dtype == object:\n",
    "    events_df['VT_Start'] = pd.to_datetime(events_df['VT_Start'])\n",
    "\n",
    "if 'desc_lookup' not in globals():\n",
    "    desc_lookup = (\n",
    "        events_df.dropna(subset=['Description'])\n",
    "        .drop_duplicates(subset=['Source', 'VT_Start'])\n",
    "        .set_index(['Source', 'VT_Start'])['Description']\n",
    "    )\n",
    "\n",
    "def normalize_action_type(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    return str(val).strip()\n",
    "\n",
    "def get_action_type(source, ts):\n",
    "    key = (source, ts)\n",
    "    try:\n",
    "        return normalize_action_type(desc_lookup.loc[key])\n",
    "    except KeyError:\n",
    "        return None\n",
    "\n",
    "def get_recommended_action_type(rec):\n",
    "    rec_source = rec['recommended_action_source']\n",
    "    rec_dir = int(rec['recommended_action_direction'])\n",
    "    rec_mag = rec['recommended_action_magnitude']\n",
    "    if pd.isna(rec_mag):\n",
    "        return None\n",
    "    hist_match = context_df_clean[\n",
    "        (context_df_clean['episode_id'] == rec['matched_episode_id']) &\n",
    "        (context_df_clean['action_source'] == rec_source) &\n",
    "        (context_df_clean['action_direction'] == rec_dir) &\n",
    "        (np.isclose(context_df_clean['action_magnitude'], rec_mag, atol=1e-6))\n",
    "    ]\n",
    "    if len(hist_match) == 0:\n",
    "        return None\n",
    "    hist_ts = pd.to_datetime(hist_match.iloc[0]['action_timestamp'])\n",
    "    return get_action_type(rec_source, hist_ts)\n",
    "\n",
    "def compute_match_stats(episode_id, tol):\n",
    "    ep_row = episodes_operated_tags_df[episodes_operated_tags_df['EpisodeID'] == episode_id].iloc[0]\n",
    "    alarm_start = ep_row['AlarmStart']\n",
    "    alarm_end = ep_row['AlarmEnd']\n",
    "\n",
    "    actual_actions, _ = get_operator_actions_for_episode(\n",
    "        alarm_start, alarm_end, target_sources=['03LIC_1071', '03LIC_1016', '03PIC_1013']\n",
    "    )\n",
    "    actual_actions = clean_actual_actions(actual_actions).sort_values('VT_Start')\n",
    "\n",
    "    if len(actual_actions) == 0:\n",
    "        return {\n",
    "            'episode_id': episode_id,\n",
    "            'actual_actions': 0,\n",
    "            'matched_actions': 0,\n",
    "            'match_pct': None\n",
    "        }\n",
    "\n",
    "    minute_df = load_minute_results(episode_id)\n",
    "    top3_df = minute_df[minute_df['rank'].isin([1, 2, 3])].copy()\n",
    "\n",
    "    matched = 0\n",
    "    for _, act in actual_actions.iterrows():\n",
    "        act_time = pd.to_datetime(act['VT_Start'])\n",
    "        act_source = act['Source']\n",
    "        act_dir = int(act['action_direction'])\n",
    "        act_mag = float(act['action_magnitude'])\n",
    "        act_type = normalize_action_type(act.get('Description', None))\n",
    "\n",
    "        # Find closest recommendation time\n",
    "        time_diff = (top3_df['current_time'] - act_time).abs().dt.total_seconds()\n",
    "        closest_idx = time_diff.idxmin()\n",
    "        closest_time = top3_df.loc[closest_idx, 'current_time']\n",
    "        closest_recs = top3_df[top3_df['current_time'] == closest_time]\n",
    "\n",
    "        # Match if any of top-3 has same tag, direction, magnitude within tolerance, and action type\n",
    "        is_match = False\n",
    "        for _, rec in closest_recs.iterrows():\n",
    "            rec_source = rec['recommended_action_source']\n",
    "            rec_dir = int(rec['recommended_action_direction'])\n",
    "            rec_mag = rec['recommended_action_magnitude']\n",
    "            rec_type = get_recommended_action_type(rec)\n",
    "            if pd.isna(rec_mag):\n",
    "                continue\n",
    "            if act_type is None or rec_type is None:\n",
    "                continue\n",
    "            if (\n",
    "                rec_source == act_source\n",
    "                and rec_dir == act_dir\n",
    "                and abs(rec_mag - act_mag) <= tol\n",
    "                and rec_type == act_type\n",
    "            ):\n",
    "                is_match = True\n",
    "                break\n",
    "        if is_match:\n",
    "            matched += 1\n",
    "\n",
    "    total = len(actual_actions)\n",
    "    return {\n",
    "        'episode_id': episode_id,\n",
    "        'actual_actions': total,\n",
    "        'matched_actions': matched,\n",
    "        'match_pct': (matched / total) * 100 if total > 0 else None\n",
    "    }\n",
    "\n",
    "# Compute per-episode stats\n",
    "per_episode_stats = [compute_match_stats(ep_id, magnitude_tolerance) for ep_id in episode_ids]\n",
    "stats_df = pd.DataFrame(per_episode_stats)\n",
    "\n",
    "print(\"=== Tag + Direction + Magnitude + Action Type Match (per episode) ===\")\n",
    "print(stats_df.to_string(index=False))\n",
    "\n",
    "# Overall stats across all episodes\n",
    "total_actions = stats_df['actual_actions'].sum()\n",
    "total_matched = stats_df['matched_actions'].sum()\n",
    "overall_pct = (total_matched / total_actions) * 100 if total_actions > 0 else None\n",
    "\n",
    "print(\"\\n=== Overall Match Across All Episodes ===\")\n",
    "print(f\"Total actual actions: {total_actions}\")\n",
    "print(f\"Matched actions: {total_matched}\")\n",
    "if overall_pct is not None:\n",
    "    print(f\"Overall match % (tol={magnitude_tolerance}): {overall_pct:.2f}%\")\n",
    "else:\n",
    "    print(\"Overall match %: N/A (no actions)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5968ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adnoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
